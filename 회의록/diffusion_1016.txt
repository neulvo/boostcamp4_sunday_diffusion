학습 방향

멘토님께 커리큘럼 받기 -> 직접 돌려 보기
공부한 내용 포스팅, 피피티 만들기
레포파는 것도 하나의 도전

성현 님 - variance bound를 구하는 것, 포워드 리버스, 평균을 구하는 게 아니라 Langevin 수식 써서 epsilon을 예측하고 오차를 down-weight 방식으로 계산하며 학습하는 모델이다.

상모 님 -
1. 백그라운드 포워드, 리버스 수식의 구성과 마르코프 체인
2. 막힌 부분 - objective function 커스텀한 부분, 마르코프 체인 수식 전개 / loss 설정 - KLD 부분을 보완해서 보고 싶다. 
3. 디퓨젼 해석이나 bits, dim과도 연결되는듯, ddpm이 뭐하는건데?
    1. 열역학과 연관성, 스코어, 정보이론

현수 님 - 
1. VAE 부터 완벽하게 이해하고 봐야겠다.
2. 이미지 데이터 - VAE 하나의 latent vector를 뉴럴 네트워크로 학습시켰다면 이거는 1000번의 스텝 동안 Gausian 노이즈를 추가하며 키워가며 결국에는 완벽한 이미지에서 완전 노이즈로 가득한 xt로 완성시키고 그걸 다시 리버스하는 곳에서 딥러닝 학습을 해서 이미지를 만든다고 이해를 했다.
3. 논문 3쪽의 로스 함수 정의에서 xt랑 x0까지 주어져 있는데 마르코프 관점에서 이전 타임스탭만 참고해야하니 0가 있으면 안되는 거 아닌가 근데 이건 포워드 스탭이고 리버스 스탭일 때만 마르코프 체인을 따지는 건가?? 0이 이해가 안됨
    답변 1. 0에서 티마이너스1까지 따지는 건 아니고 x0과 xt | xt-1의 교집합을 생각한다고 보면 될 것 같습니다.
          2-1. 정규분포의 리버스가 Gausian이라는 것이 알려져 있기 떄문에 ptheta를 Gausian이라고 가정하고 시작하는 거고 맨 마지막 Gausian t에서 0까지 가는 확률을 극대화해야 하기 때문에 KL divergence를 추가하는 거예요.
	 KLD를 인공적인 텀으로 두고 시작, 그거를 이제 쪼개보는 거예요. 오리지날 분포와 ptheta의 분포가 같은지를 KLD로 비교하며 쪼개가면 L~로 분해가 되는 거거든요
          2-2. 핵심적인 로직은 ptheta를 저희가 알고자하는건데 q x1부터 t까지 x0로부터 q의 분포랑 ptheta의 분포가 가까워지게끔 KLD를 분해해가며 KLD의 합으로 표현이 될 수 있다. (중간에 놓침)
          3. x0가 궁극적인 목적, x0를 다시 만들어야 한다. 그 x0가 나올 확률을 극대화하는 방향으로 학습 
4. loss 함수나 수식에 대한 보완이 필요하다
5. VAE이랑 많이 비교하는데 명확히 아는 게 아니구나

규보님 -  결국은 이미지를 하나 잡고 노이즈로 가우시안 분포를 줘서 1000 단계를 거쳐서 알아보기 힘든 이미지 상태로 만드는데 그때 그 가우시안 분포를 분산 값을 베타t로 저희가 이미 설정한 값을 줘서 
	디퓨전을 시킨 다음 최대한 이 분포를 맞춰서 백시키기 위해 확률 분포를 학습시킨다 느꼈고요. 그 VAE를 사용한 loss를 갖고 와서 이걸 DDPM 식의 loss로 바꾸는 게 핵심인 아이디어인 것 같더라고요 
	그 속에서 결국 ptheta를 완벽하게 알아내기는 힘드니까 디퓨전을 다시 리버스하는 과정을 이것도 가우시안 분포를 따른다는 관찰을 적용을 해서 
	그럼 분산과 평균을 결정해줘야 되는데 분산은 저희가 아까 디퓨전 시킬 때 아는 값 베타를 이용해서 고정을 시켜버리고 평균도 원래는 식이 되게 복잡한데 이걸 x0와 epsilon에 대한 식으로 바꿔버리고 
	결국 epsilon을 찾는 문제로 바꿔버리더라고요 자세한 과정은 모르지만 요 자그마한 확률변수인 epsilon만 찾아도? 상당히 좋은 이미지가 얻어지는구나 이런 느낌을 받았습니다.

인희님 - 일단 포워드 과정에서는 이미지가 주어졌을 때 가우시안 -> 노이지안 상태 -> 리버스 -> 학습 -> 복원
         - VAE 로스에서 로스 변환 수식을 보고 있는 중
         - 각 타임스텝에서 다양한 Gausian 노이즈를 예측해서 denoising에 활용하자

방인서 님 - 백그라운드까지 2시간 보고 유투브 정독하고 3시간이 지났는데
- x0가 왜 들어가는지가 의문이었다, 생각해봐야겠다
- 생성모델을 GAN 이후에 처음 본, 이게 왜 잘되지? 스텝만 쪼갠 느낌?? 성능 퍼포먼스보다 시작에 의의가 있지 않았나
- 하고 싶은 얘기나 큰 흐름은 이해하겠다 좀 더 봐야겠다

그 외)
- VAE, GAN과 차이를 명확하게 알아야겠다.
- 트랜스포머 사인파가 반가웠다.

규보님 
- 정확히 확률 분포가 무슨 확률 변수길래 제대로 깨끗한 이미지가 뿌연 이미지로 변화해가는건지 그 과정이 이해가 안됐어요.
- x0가 데이터에서 시작하니까 figure를 보시면 스위스롤 데이터, 이미지 데이터 다양한 데이터 엑스 와이의 데이터 분포로 시작할 거고, 이미지 데이터면 이미지들의 픽셀 값들의 분포가 아닐까요?
- 이미지를 데이터가 아닌 확률 분포로 볼 수 있으니까
- 베타 t를 이용한 Gausian 분포에서의 노이즈? Gausian 노이즈를 더하는 게 데이터 분포 공간을 어떻게 바꾸는 건지?
- RGB 값이 확률 변수인가요?
- 매니폴드가 정해진 공간에 있는데 그걸 확산시켜나가는 것
- 비트로 변환하는 과정이 연관되지 않을까요? 
- 더해주는 게 어떤 물리량에 더하는지를 모르겠어요
- 결국엔 x, y에 따른 알지비값이 아닐까요?
- 그 값이 변하니까 노이즈가 되는 것

다음번 만남까지 뭘 해올 건가?
- 발표 준비?
- VAE?
- 고민해보고 얘기해보자.

고생하셨습니다.
